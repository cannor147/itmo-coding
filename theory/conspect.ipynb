{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Теория кодирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Терминология\n",
    "\n",
    "Пусть у нас есть два пространства слов $\\mathbb{W}$ и $\\mathbb{C}$ над алфавитом $\\mathbb{Q}$. Тогда код это некоторое подпространство слов $\\mathcal{C} \\subseteq \\mathbb{C}$, для которого существует биекция (или хотя бы сюръекция) из $\\mathbb{W}$.\n",
    "\n",
    "В теории кодирования изучается следующая модель:\n",
    "\n",
    "1. Источник: $w \\in \\mathbb{W}$\n",
    "2. Кодирование: $\\operatorname{encoding} : \\mathbb{W} \\rightarrow \\mathcal{C}$, $c = \\operatorname{encoding}(w)$\n",
    "3. Модуляция: $\\operatorname{modulating} : \\mathbb{C} \\leftrightarrow \\mathbb{X}^*$, $x = \\operatorname{modulating}(c)$\n",
    "4. Передача данных по каналу: $\\operatorname{transfer} : \\mathbb{X}^* \\rightarrow \\mathbb{Y}^*$, $y = \\operatorname{transfer}(x)$\n",
    "5. Демодуляция: $\\operatorname{demodulating} : \\mathbb{Y}^* \\leftrightarrow \\hat{\\mathbb{C}}$, $\\hat{c} = \\operatorname{demodulating}(y)$\n",
    "6. Декодирование: $\\operatorname{decoding} : \\hat{\\mathbb{C}} \\rightarrow \\hat{\\mathbb{W}}$, $\\hat{w} = \\operatorname{decoding}(\\hat{c})$\n",
    "\n",
    "Если $|\\mathbb{W}| \\le |\\mathbb{C}|$, то такую модель изучает теория помехоустойчивого кодирования. Если же $|\\mathbb{W}| \\ge |\\mathbb{C}|$, то такую модель изучает теория сжатия данных. В данном курсе рассматривается только теория помехоустойчивого кодирования."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Кодирование\n",
    "\n",
    "Суть кодирования заключается в том, чтобы сопоставить слову из $\\mathbb{W}$ некоторое слово из $\\mathbb{C}$ таким образом, чтобы при декодировании целые группы слов из $\\mathbb{C}$ превращались в одно слово из $\\mathbb{W}$. Величина такой группы слов характеризует избыточность кода."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Систематическое и несистематическое кодирование\n",
    "\n",
    "Систематическим кодированием называется такое кодирование, что $w$ всегда является подпоследовательностью $c$. В противном случае кодирование является несистематическим. При систематическом кодировании избыточные символы отделены от информационных, при несистематическом же избыточность вносится на уровне всего слова."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Декодирование"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Посимвольное и поблоковое декодирование\n",
    "\n",
    "В зависимости от принципа работы декодера, можно различать:\n",
    "\n",
    "* Посимвольное декодирование $\\operatorname{decoding} : \\hat{\\mathbb{C}} \\rightarrow \\mathbb{Q}^*$, при котором результат даже не обязан быть из $\\mathbb{W}$ (но может и быть обязан)\n",
    "* Поблоковое декодирование $\\operatorname{decoding} : \\hat{\\mathbb{C}} \\rightarrow \\mathbb{W}$\n",
    "* Списочное декодирование $\\operatorname{decoding} : \\hat{\\mathbb{C}} \\rightarrow \\mathbb{W}^*$, при котором результат это группа ближайших слов из $\\mathbb{W}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Жёсткое и мягкое декодирование\n",
    "\n",
    "Введём понятие вероятностного множества $P(\\mathbb{A})$ — т.е. вероятности встречи для каждого слова из $\\mathbb{A}$. Тогда:\n",
    "\n",
    "* В демодуляторе различают жёсткое детектирование при $\\hat{\\mathbb{C}} = \\mathbb{C}$ и мягкое детектирование при $\\hat{\\mathbb{C}} = P(\\mathbb{C})$.\n",
    "* В декодере различают жёсткое декодирование при $\\hat{\\mathbb{C}} = \\mathbb{C}$ и мягкое декодирование при $\\hat{\\mathbb{C}} = P(\\mathbb{C})$.\n",
    "* В декодере различают декодирование с жёстким выходом при $\\hat{\\mathbb{W}} = \\mathbb{W}$ и декодирование с мягким выходом при $\\hat{\\mathbb{W}} = P(\\mathbb{W})$.\n",
    "\n",
    "Такие разновидности декодеров позволяют комбинировать их друг с другом."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Критерии декодирования\n",
    "\n",
    "Естественным критерием качества декодирования является вероятность несовпадения оценки $\\hat{x}$ и истинного сообщения $x$. Если $y$ — принятый вектор, то такая вероятность ошибки равна:\n",
    "\n",
    "$$P_{error} = P \\{x \\neq \\hat{x}\\} = \\sum_{x \\neq \\hat{x}} P_{X|Y} \\{x|y\\} = 1 - P_{X|Y} \\{\\hat{x}|y\\}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Критерий максимума апостериорной вероятность (критерий идеального наблюдателя)\n",
    "\n",
    "Очевидно, что минимальная вероятность ошибки достигается при выборе:\n",
    "\n",
    "$$\\hat{x} = \\underset{x}{\\operatorname{argmax}} P_{X|Y} \\{x|y\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Критерий максимума правдоподобия\n",
    "\n",
    "$$\\hat{x} = \\underset{x}{\\operatorname{argmax}} P_{Y|X} \\{y|x\\}$$\n",
    "\n",
    "В общем случае, данные два критерия не являются эквивалентными. Однако если все сообщения равновероятны (т.е. $\\forall x . P_X \\{x\\} = s$), тогда по теореме Байеса можно вывести, что\n",
    "\n",
    "$$\n",
    "\\underset{x}{\\operatorname{argmax}} P_{Y|X} \\{y|x\\} =\n",
    "\\underset{x}{\\operatorname{argmax}} \\frac{P_{X|Y} \\{x|y\\} P_Y \\{y\\}}{P_X \\{x\\}} =\n",
    "\\underset{x}{\\operatorname{argmax}} \\frac{P_{X|Y} \\{x|y\\}}{s} =\n",
    "\\underset{x}{\\operatorname{argmax}} P_{X|Y} \\{x|y\\}\n",
    "$$\n",
    "\n",
    "При посимвольном декодировании верно для каналов без памяти:\n",
    "\n",
    "$$\n",
    "\\hat{x} =\n",
    "\\underset{x}{\\operatorname{argmax}} P_{Y^n|X^n} \\{y|x\\} =\n",
    "\\underset{x}{\\operatorname{argmax}} P_{Y^n|X^n} \\{y_1, y_2, \\dots y_n|x_1, x_2, \\dots x_n\\} =\n",
    "\\underset{x}{\\operatorname{argmax}} \\prod_{i=1}^{n} P_{Y|X} \\{y_i|x_i\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Логарифмическое отношение правдоподобия\n",
    "\n",
    "В случае $\\mathbb{X} = \\{a, b\\}$ часто полезно оценить надёжность оценки для каждого символа через логарифмическое отношение:\n",
    "\n",
    "$$L = \\ln\\frac{P \\{x_i=a|y\\}}{P \\{x_i=b|y\\}} = \\ln\\frac{\\sum_{x|x_i=a} P_{X|Y} \\{x|y\\}}{\\sum_{x|x_i=b} P_{X|Y} \\{x|y\\}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Логарифмическое отношение правдоподобия в аддитивном гауссовском канале\n",
    "\n",
    "$$\n",
    "L =\n",
    "\\ln\\frac{P \\{x_i=-1|y\\}}{P \\{x_i=1|y\\}} =\n",
    "\\ln\\frac{P \\{y|x_i=-1\\}}{P \\{y|x_i=1\\}} =\n",
    "\\ln\\frac{\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\cdot \\exp{-\\frac{(y_i + 1)^2}{2\\sigma^2}}}{\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\cdot \\exp{-\\frac{(y_i - 1)^2}{2\\sigma^2}}} =\n",
    "\\ln\\exp\\frac{(y_i - 1)^2-(y_i + 1)^2}{2\\sigma^2} =\n",
    "-\\frac{2y_i}{\\sigma^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Критерий минимального расстояния\n",
    "\n",
    "Введём функцию расстояния $d(x,y) : \\mathbb{Y} \\times \\mathbb{Y} \\rightarrow \\mathbb{D}$, где $\\mathbb{D} \\subseteq \\mathbb{R}_{+}$. Тогда критерием минимального расстояния будет являться:\n",
    "\n",
    "$$\\hat{x} = \\underset{x}{\\operatorname{argmin}} d(y,x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Критерий минимального расстояния Хэмминга в двоичном симметричном канале\n",
    "\n",
    "В случае двоичного симметричного канала если $p \\le \\frac{1}{2}$, то критерий минимального расстояния Хэмминга эквивалентен критерию максимума правдоподобия:\n",
    "\n",
    "$$\n",
    "\\underset{x}{\\operatorname{argmax}}P(y | x) =\n",
    "\\underset{x}{\\operatorname{argmax}} \\sum_{i=1}^{n} (1-p) \\Bigr(\\frac{p}{1-p}\\Bigl)^{\\delta(y_i, x_i)} \\sim\n",
    "\\underset{x}{\\operatorname{argmax}} \\sum_{i=1}^{n} \\Bigr(1-\\frac{1}{1-p}\\Bigl)^{\\delta(y_i, x_i)} \\sim\n",
    "\\underset{x}{\\operatorname{argmin}} \\sum_{i=1}^{n} \\delta(y_i, x_i) =\n",
    "\\underset{x}{\\operatorname{argmin}} d_H(y, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Критерий минимального Евклидового расстояния в аддитивном гауссовском канале\n",
    "\n",
    "В случае канала с АБГШ критерий минимального Евклидового расстояния эквивалентен критерию максимума правдоподобия:\n",
    "\n",
    "$$\n",
    "\\underset{x}{\\operatorname{argmax}} P(y | x) =\n",
    "\\underset{x}{\\operatorname{argmax}} \\prod_{i = 0}^{n- 1} \\frac{\\exp{-\\frac{(y_i - x_i)^2}{2\\sigma^2}}}{\\sqrt{2 \\pi \\sigma^2}} \\sim\n",
    "- \\underset{x}{\\operatorname{argmax}} \\sum_{i = 0}^{n - 1} \\frac{(y_i - x_i)^2}{2\\sigma^2} \\sim\n",
    "\\underset{x}{\\operatorname{argmin}} \\sum_{i = 0}^{n - 1} (y_i - x_i)^2 \\sim\n",
    "\\underset{x}{\\operatorname{argmin}}d_E(y, x)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
